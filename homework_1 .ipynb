{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "raw",
      "source": "First we define a function to extract columns to be used for this project and drop rows with NAN values",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# function to extract columns to be used for this project and drop rows with NAN values\ndef reduce_and_convert(df):\n    # extract the columns by column name\n    df = df[['room_type', 'accommodates', 'bathrooms', 'bedrooms', 'host_is_superhost',\n             'beds', 'bed_type', 'price', 'cleaning_fee', 'cancellation_policy',\n             'review_scores_value', 'review_scores_location', 'host_identity_verified']]\n\n    # replace certain characters to allow for data type conversion\n    df['price'] = df['price'].str.replace('$','')\n    df['price'] = df['price'].str.replace(',','')\n    df['cleaning_fee'] = df['cleaning_fee'].str.replace('$','')\n    df['cleaning_fee'] = df['cleaning_fee'].str.replace(',','')\n    df['cleaning_fee'] = df['cleaning_fee'].fillna(0.0)\n    df['host_is_superhost'].map({'t': True, 'f': False})\n    df['host_identity_verified'].map({'t': True, 'f': False})\n\n    # convert data types for certain columns so they are not categorical anymore\n    df = df.astype({'price':'float'})\n    df = df.astype({'cleaning_fee':'float'})\n    df = df.astype({'host_is_superhost':'bool'})\n    df = df.astype({'host_identity_verified':'bool'})\n\n    # drop any rows with NAN entries\n    df = df.dropna(axis=0)\n\n    return df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "Next we need a function to convert categorical columns into usable ones. Here we would also fill NAN values with the mean etc. however for this project we don't need this.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# convert categorical columns\ndef add_mean_and_dumnmy(df):\n    # get column names of categorical columns\n    cat_vars = df.select_dtypes(include=['object']).columns\n\n    # convert categorical columns column by column\n    for var in cat_vars:\n        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)],\n                       axis=1)\n\n    return df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "We need a function to create a heatmap of a selected subset of columns to answer questions 1.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# show heatmap of correlation between a subset of columns \ndef plot_hist_and_heatmap(df):\n    # select subset of data\n    df = df[['price', 'cleaning_fee', 'number_of_reviews', 'review_scores_rating', 'accommodates', 'bathrooms', 'bedrooms']]\n    # create heatmap\n    sns.heatmap(df.corr(), annot=True, fmt=\".2f\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "And a function to create the list of top correlating variables with price.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# train a linear regression model \ndef train_model_and_coeffs(df):\n    # split data into X and y\n    X = df.drop(['price'], axis=1)\n    y = df['price']\n    \n    # use udacity data scientist provided code to train the model and search for optimal model\n    cutoffs = [5000, 3500, 2500, 1000, 100, 50]\n    r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test = t.find_optimal_lm_mod(X, y, cutoffs)\n\n    # look at model coefficients\n    coefs_df = pd.DataFrame()\n    coefs_df['est_int'] = X_train.columns\n    coefs_df['coefs'] = lm_model.coef_\n    coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n\n    coefs_df.head(20)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "Main program call. Importing all needed packages and executing functions for both cities.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# read in needed packages\n# import warnings\n# warnings.simplefilter(action='ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport AllTogether as t\nimport seaborn as sns\n%matplotlib inline\n\n# read in BOSTON and SEATTLE airbnb data sets and process using the previously written functions\ndef main():\n    # boston\n    df = pd.read_csv('./boston/listings.csv')\n    df = reduce_and_convert(df)\n    df = add_mean_and_dumnmy(df)\n    print('BOSTON')\n    train_model_and_coeffs(df)\n\n    # seattle\n    df = pd.read_csv('./seattle/listings.csv')\n    df = reduce_and_convert(df)\n    df = add_mean_and_dumnmy(df)\n    print('SEATTLE')\n    train_model_and_coeffs(df)\n\nmain()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}