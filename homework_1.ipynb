{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Project: Write a Data Science Blog Post"
      ],
      "metadata": {
        "id": "ru2Bm1FCHOLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 1: Business Understanding\n",
        "\n",
        "This contains the code for homework 1 in the Udacity Data Scientist Nano Degree. The motivation behind this is to pass the course. As such I decided to look at the provided data sets of airbnb listing data in Bosten and Seattle to answer the following questions:\n",
        "\n",
        "**Question 1**: What is the correlation between the following values: 'price', 'cleaning_fee', 'review_scores_location', 'review_scores_value', 'accommodates', 'bathrooms', 'bedrooms'\n",
        "\n",
        "**Question 2**: Which of the following variables has the strongest correlation with the listing price: 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'host_is_superhost', 'beds', 'bed_type', 'cleaning_fee', 'cancellation_policy', 'review_scores_value', 'review_scores_location', 'host_identity_verified'\n",
        "\n",
        "**Question 3**: Compare the results of questions 1 and 2 between Bosten and Seattle to look for interesting similarities/differences."
      ],
      "metadata": {
        "id": "b3cQQoqOHULl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation\n",
        "\n",
        "The following function extracts the columns to be used for this project"
      ],
      "metadata": {
        "id": "1kEHdR6HOgXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract columns to be used for this project and drop rows with NAN values\n",
        "def reduce_and_convert(df):\n",
        "    # extract the columns by column name\n",
        "    df = df[['room_type', 'accommodates', 'bathrooms', 'bedrooms', 'host_is_superhost',\n",
        "             'beds', 'bed_type', 'price', 'cleaning_fee', 'cancellation_policy',\n",
        "             'review_scores_value', 'review_scores_location', 'host_identity_verified']]\n",
        "\n",
        "    # replace certain characters to allow for data type conversion\n",
        "    df['price'] = df['price'].str.replace('$','')\n",
        "    df['price'] = df['price'].str.replace(',','')\n",
        "    df['cleaning_fee'] = df['cleaning_fee'].str.replace('$','')\n",
        "    df['cleaning_fee'] = df['cleaning_fee'].str.replace(',','')\n",
        "    df['cleaning_fee'] = df['cleaning_fee'].fillna(0.0)\n",
        "    df['host_is_superhost'].map({'t': True, 'f': False})\n",
        "    df['host_identity_verified'].map({'t': True, 'f': False})\n",
        "\n",
        "    # convert data types for certain columns so they are not categorical anymore\n",
        "    df = df.astype({'price':'float'})\n",
        "    df = df.astype({'cleaning_fee':'float'})\n",
        "    df = df.astype({'host_is_superhost':'bool'})\n",
        "    df = df.astype({'host_identity_verified':'bool'})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "dHrP_fU7HDul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further data preparation is needed so this function is used to drop rows with NAN values and convert categorical columns into ones that can be used by our model. Alternatively to dropping rows we could fill NAN values with the mean etc. however for this project we decided against this."
      ],
      "metadata": {
        "id": "hNt9RX8bOkVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert categorical columns and drop rows containing nan\n",
        "def add_mean_and_dumnmy(df):\n",
        "    # drop any rows with NAN entries\n",
        "    df = df.dropna(axis=0)\n",
        "\n",
        "    # get column names of categorical columns\n",
        "    cat_vars = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # convert categorical columns column by column\n",
        "    for var in cat_vars:\n",
        "        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)],\n",
        "                       axis=1)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "3wEfsyZJHDus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Modeling \n",
        "\n",
        "Here we fit a linear regression model using Udacity provided code."
      ],
      "metadata": {
        "id": "f7XPS2pSOmbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(df):\n",
        "    # split data into X and y\n",
        "    X = df.drop(['price'], axis=1)\n",
        "    y = df['price']\n",
        "    \n",
        "    # use udacity data scientist provided code to train the model and search for optimal model\n",
        "    cutoffs = [5000, 3500, 2500, 1000, 100, 50]\n",
        "    r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test = t.find_optimal_lm_mod(X, y, cutoffs)\n",
        "\n",
        "    return r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "N8klnK0sJaPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation\n",
        "\n",
        "This function alows us to plot a heatmap visualizing the correlation between a list of columns. **This is used to answer question 1 and 3**. With a theoretical range of -1 to 1 the numbers in the resulting heatmap indicate how two variables correlate. A value of 1 would indicate perfect positive correlation meaning a doubling of variable x results in a doubling of variable y. A value close to 0 indicates little to no correlation."
      ],
      "metadata": {
        "id": "pdAx6pq8Joan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show heatmap of correlation between a subset of columns \n",
        "def plot_hist_and_heatmap(df, col_list):\n",
        "    # select subset of data\n",
        "    df = df[col_list]\n",
        "    # create heatmap\n",
        "    sns.heatmap(df.corr(), annot=True, fmt=\".2f\")"
      ],
      "metadata": {
        "id": "Wh6NCg7_HDuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is used to list the top x coefficients of a given lm model. **This is used to answer question 2 and 3**. Since the data for our model was normalized the resulting values list of coefficients represends the input variables with the greatest correlation to the output/predicted variable (in our case this would be the listing price). The sign of the coefs column indicates positive or negative correlation.  "
      ],
      "metadata": {
        "id": "Ci7e_0gUKN7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract coefs from lm model an show top results\n",
        "def print_coeffs(lm_model, X_train, top_x_results = 10):\n",
        "\n",
        "    # look at model coefficients\n",
        "    coefs_df = pd.DataFrame()\n",
        "    coefs_df['est_int'] = X_train.columns\n",
        "    coefs_df['coefs'] = lm_model.coef_\n",
        "    coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n",
        "    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n",
        "\n",
        "    coefs_df.head(top_x_results)"
      ],
      "metadata": {
        "id": "rTXrNzW0HDux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main program call. Importing all needed packages and executing functions for both cities to get all the desired answers."
      ],
      "metadata": {
        "id": "YMyWhJPUOo5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in needed packages\n",
        "# import warnings\n",
        "# warnings.simplefilter(action='ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import AllTogether as t\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# read in BOSTON and SEATTLE airbnb data sets and process using the previously written functions\n",
        "def main():\n",
        "    # boston\n",
        "    # read dataset\n",
        "    df = pd.read_csv('./boston/listings.csv')\n",
        "    # prepare data\n",
        "    df = reduce_and_convert(df)\n",
        "    df = add_mean_and_dumnmy(df)\n",
        "    # train a model\n",
        "    r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test = train_model(df)\n",
        "    # plot heatmap to answer questions 1 and 3\n",
        "    plot_hist_and_heatmap(df, ['price', 'cleaning_fee', 'number_of_reviews', 'review_scores_rating', 'accommodates', 'bathrooms', 'bedrooms'])\n",
        "    # print model coefficients to answer questions 2 and 3\n",
        "    print('BOSTON')\n",
        "    train_model_and_coeffs(lm_model, X_train)\n",
        "\n",
        "    # repeat for seattle dataset\n",
        "    df = pd.read_csv('./seattle/listings.csv')\n",
        "    df = reduce_and_convert(df)\n",
        "    df = add_mean_and_dumnmy(df)\n",
        "    r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test = train_model(df)\n",
        "    plot_hist_and_heatmap(df, ['price', 'cleaning_fee', 'number_of_reviews', 'review_scores_rating', 'accommodates', 'bathrooms', 'bedrooms'])\n",
        "    print('SEATTLE')\n",
        "    train_model_and_coeffs(lm_model, X_train)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "UhNJT9pZHDuz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}