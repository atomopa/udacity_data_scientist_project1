{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Project: Write a Data Science Blog Post"
      ],
      "metadata": {
        "id": "ru2Bm1FCHOLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 1: Business Understanding\n",
        "\n",
        "This contains the code for homework 1 in the Udacity Data Scientist Nano Degree. The motivation behind this is to pass the course. As such I decided to look at the provided data sets of airbnb listing data in Bosten and Seattle to answer the following questions:\n",
        "\n",
        "**Question 1**: What is the correlation between the following values: 'price', 'cleaning_fee', 'review_scores_location', 'review_scores_value', 'accommodates', 'bathrooms', 'bedrooms'\n",
        "\n",
        "**Question 2**: Which of the following variables has the strongest correlation with the listing price: 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'host_is_superhost', 'beds', 'bed_type', 'cleaning_fee', 'cancellation_policy', 'review_scores_value', 'review_scores_location', 'host_identity_verified'\n",
        "\n",
        "**Question 3**: Compare the results of questions 1 and 2 between Bosten and Seattle to look for interesting similarities/differences."
      ],
      "metadata": {
        "id": "b3cQQoqOHULl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 2: Data Understanding"
      ],
      "metadata": {
        "id": "TPtUTnFLfN6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the needed packages for this code"
      ],
      "metadata": {
        "id": "4xzDshRWfhvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in needed packages\n",
        "# import warnings\n",
        "# warnings.simplefilter(action='ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import AllTogether as t\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-MdCKxiYfgrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the datasets and take a look at their shape to verify."
      ],
      "metadata": {
        "id": "X2CaP8s2fwX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_boston = pd.read_csv('./boston/listings.csv')\n",
        "df_seattle = pd.read_csv('./seattle/listings.csv')\n",
        "\n",
        "df_boston.shape\n",
        "df_seattle.shape"
      ],
      "metadata": {
        "id": "TNhLay8Zf5C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 3: Data Preparation\n",
        "\n",
        "The following function extracts the columns to be used for this project"
      ],
      "metadata": {
        "id": "1kEHdR6HOgXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract columns to be used for this project and drop rows with NAN values\n",
        "def reduce_and_convert(df):\n",
        "    # extract the columns by column name\n",
        "    df = df[['room_type', 'accommodates', 'bathrooms', 'bedrooms', 'host_is_superhost',\n",
        "             'beds', 'bed_type', 'price', 'cleaning_fee', 'cancellation_policy',\n",
        "             'review_scores_value', 'review_scores_location', 'host_identity_verified']]\n",
        "\n",
        "    # replace certain characters to allow for data type conversion\n",
        "    df['price'] = df['price'].str.replace('$','')\n",
        "    df['price'] = df['price'].str.replace(',','')\n",
        "    df['cleaning_fee'] = df['cleaning_fee'].str.replace('$','')\n",
        "    df['cleaning_fee'] = df['cleaning_fee'].str.replace(',','')\n",
        "    df['cleaning_fee'] = df['cleaning_fee'].fillna(0.0)\n",
        "    df['host_is_superhost'].map({'t': True, 'f': False})\n",
        "    df['host_identity_verified'].map({'t': True, 'f': False})\n",
        "\n",
        "    # convert data types for certain columns so they are not categorical anymore\n",
        "    df = df.astype({'price':'float'})\n",
        "    df = df.astype({'cleaning_fee':'float'})\n",
        "    df = df.astype({'host_is_superhost':'bool'})\n",
        "    df = df.astype({'host_identity_verified':'bool'})\n",
        "\n",
        "    return df\n",
        "\n",
        "df_boston = reduce_and_convert(df_boston)\n",
        "df_seattle = reduce_and_convert(df_seattle)"
      ],
      "metadata": {
        "id": "dHrP_fU7HDul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further data preparation is needed so this function is used to drop rows with NAN values and convert categorical columns into ones that can be used by our model. Alternatively to dropping rows we could fill NAN values with the mean etc. however for this project we decided against this."
      ],
      "metadata": {
        "id": "hNt9RX8bOkVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert categorical columns and drop rows containing nan\n",
        "def add_mean_and_dumnmy(df):\n",
        "    # drop any rows with NAN entries\n",
        "    df = df.dropna(axis=0)\n",
        "\n",
        "    # get column names of categorical columns\n",
        "    cat_vars = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # convert categorical columns column by column\n",
        "    for var in cat_vars:\n",
        "        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)],\n",
        "                       axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "df_boston = add_mean_and_dumnmy(df_boston)\n",
        "df_seattle = add_mean_and_dumnmy(df_seattle)"
      ],
      "metadata": {
        "id": "3wEfsyZJHDus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 4: Data Modeling \n",
        "\n",
        "Here we fit a linear regression model using Udacity provided code."
      ],
      "metadata": {
        "id": "f7XPS2pSOmbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(df):\n",
        "    # split data into X and y\n",
        "    X = df.drop(['price'], axis=1)\n",
        "    y = df['price']\n",
        "    \n",
        "    # use udacity data scientist provided code to train the model and search for optimal model\n",
        "    cutoffs = [5000, 3500, 2500, 1000, 100, 50]\n",
        "    r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test = t.find_optimal_lm_mod(X, y, cutoffs)\n",
        "\n",
        "    return r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test\n",
        "\n",
        "r2_scores_test_boston, r2_scores_train_boston, lm_model_boston, X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_model(df_boston)\n",
        "r2_scores_test_seattle, r2_scores_train_seattle, lm_model_seattle, X_train_seattle, X_test_seattle, y_train_seattle, y_test_seattle = train_model(df_seattle)"
      ],
      "metadata": {
        "id": "N8klnK0sJaPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 5: Evaluation\n",
        "\n",
        "This function alows us to plot a heatmap visualizing the correlation between a list of columns. **This is used to answer question 1 and 3**. With a theoretical range of -1 to 1 the numbers in the resulting heatmap indicate how two variables correlate. A value of 1 would indicate perfect positive correlation meaning a doubling of variable x results in a doubling of variable y. A value close to 0 indicates little to no correlation."
      ],
      "metadata": {
        "id": "pdAx6pq8Joan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show heatmap of correlation between a subset of columns \n",
        "def plot_hist_and_heatmap(df, col_list):\n",
        "    # select subset of data\n",
        "    df = df[col_list]\n",
        "    # create heatmap\n",
        "    sns.heatmap(df.corr(), annot=True, fmt=\".2f\")"
      ],
      "metadata": {
        "id": "Wh6NCg7_HDuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is used to list the top x coefficients of a given lm model. **This is used to answer question 2 and 3**. Since the data for our model was normalized the resulting values list of coefficients represends the input variables with the greatest correlation to the output/predicted variable (in our case this would be the listing price). The sign of the coefs column indicates positive or negative correlation.  "
      ],
      "metadata": {
        "id": "Ci7e_0gUKN7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract coefs from lm model an show top results\n",
        "def print_coeffs(lm_model, X_train, top_x_results = 10):\n",
        "\n",
        "    # look at model coefficients\n",
        "    coefs_df = pd.DataFrame()\n",
        "    coefs_df['est_int'] = X_train.columns\n",
        "    coefs_df['coefs'] = lm_model.coef_\n",
        "    coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n",
        "    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n",
        "\n",
        "    coefs_df.head(top_x_results)"
      ],
      "metadata": {
        "id": "rTXrNzW0HDux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** Ho do the variables 'price', 'cleaning_fee', 'number_of_reviews', 'review_scores_rating', 'accommodates', 'bathrooms' and 'bedrooms' correlate in the Boston dataset?"
      ],
      "metadata": {
        "id": "YMyWhJPUOo5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_hist_and_heatmap(df_boston, ['price', 'cleaning_fee', 'number_of_reviews', 'review_scores_rating', 'accommodates', 'bathrooms', 'bedrooms'])"
      ],
      "metadata": {
        "id": "UhNJT9pZHDuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Which variables show the highest correlation with the price of a listing for Boston?"
      ],
      "metadata": {
        "id": "oXjm8ecDhZzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_and_coeffs(lm_model_boston, X_train_boston)"
      ],
      "metadata": {
        "id": "1FKuUTW_hl45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** How do Boston and Seattle differ when it comes to the questions posed before."
      ],
      "metadata": {
        "id": "VriJ1l6ehtpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the heatmap for both cities to compare:"
      ],
      "metadata": {
        "id": "lRMHa8QViA6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_hist_and_heatmap(df_boston, ['price', 'cleaning_fee', 'number_of_reviews', 'review_scores_rating', 'accommodates', 'bathrooms', 'bedrooms'])\n",
        "plot_hist_and_heatmap(df_seattle, ['price', 'cleaning_fee', 'number_of_reviews', 'review_scores_rating', 'accommodates', 'bathrooms', 'bedrooms'])"
      ],
      "metadata": {
        "id": "TYiwSEEoh9nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the top 10 coefficients for both cities to compare:"
      ],
      "metadata": {
        "id": "vhG0wZKqiIZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('BOSTON')\n",
        "train_model_and_coeffs(lm_model_boston, X_train_boston)\n",
        "\n",
        "print('SEATTLE')\n",
        "train_model_and_coeffs(lm_model_seattle, X_train_seattle)"
      ],
      "metadata": {
        "id": "wxCpvoK6iPHC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}